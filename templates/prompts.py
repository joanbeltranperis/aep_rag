initial_retrieval_prompt_template = """
Eres un asistente experto en recuperación de información. A continuación tienes una lista numerada de títulos de documentos. Después se presenta una pregunta.

Tu tarea es analizar los títulos y devolver **el número de los 3**  documentos más relevantes para responder esa pregunta. Devuélvelos en orden de mayor a menor relevancia, separados por comas.

No expliques tu razonamiento. No devuelvas los títulos. No incluyas texto adicional. Solo los números.

---

TÍTULOS:
{document_titles}

---

PREGUNTA:  
{user_question}

---

RESPUESTA:

"""

answer_prompt_template = """
Eres un asistente médico especializado en vacunas. A continuación tienes una serie de fragmentos extraídos del "Manual de Inmunizaciones de la AEP".

Cada fragmento puede contener información útil para responder a una pregunta clínica.

Utiliza **únicamente** la información contenida en los fragmentos para responder la pregunta.

Incluye en tu respuesta referencias explícitas al capítulo correspondiente cuando uses información de un fragmento (por ejemplo: *según el capítulo 28. Hepatitis A*).

Si la pregunta no puede ser respondida con la información proporcionada, indícalo claramente.

Al final de la respuesta, incluye las **únicamente** las urls de los capítulos a los que se hace referencia en la repuesta. Utiliza el siguiente formato:

* Capítulo 1. Título del capítulo: [https://vacunasaep.org/documentos/manual/cap-1]
* Capítulo 2. Título del capítulo: [https://vacunasaep.org/documentos/manual/cap-2]

Pregunta del usuario:
---------------------
{question}

Fragmentos recuperados:
------------------------
{context}

Respuesta:
"""

evaluation_prompt_template = """
You are a professional evaluator assessing two answers to the same professional question: one written by a human expert, and the other generated by a retrieval-augmented generation (RAG) system.

Evaluate both answers according to the following five criteria, assigning a score from 0 to 5 for each (0 = very poor, 5 = excellent). Provide a brief justification for each score.

The response should be in JSON format, containing a dictionary with two keys: "human_evaluation" and "rag_evaluation". Each of this keys should also contain a dictionary with this five keys: "correctness", "completeness", "relevance", "clarity_and_fluency", and "alignment_with_intent". Each of these keys should have a score from 0 to 5, and a justification string. Here's an example of the expected output:

{{
  "human_evaluation": {{
    "correctness": X,
    "completeness": X,
    "relevance": X,
    "clarity_and_fluency": X,
    "alignment_with_intent": X,
    "justification": "Explain the scoring briefly."
  }},

  "rag_evaluation": {{
    "correctness": X,
    "completeness": X,
    "relevance": X,
    "clarity_and_fluency": X,
    "alignment_with_intent": X,
    "justification": "Explain the scoring briefly."
  }}
}}


### Evaluation Criteria:
1. Correctness: Is the information factually accurate?  
2. Completeness: Does the answer cover all relevant aspects of the question?  
3. Relevance: Is the content directly related to the question asked?  
4. Clarity and Fluency: Is the answer easy to understand and well-written?  
5. Alignment with Intent: Does the answer respond to the specific intent behind the question?

---

### Input:

Question:  
{question}

Human Answer:  
{human_answer}

RAG Answer:  
{rag_answer}

---

### Output:

"""
