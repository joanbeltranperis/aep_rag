initial_retrieval_prompt_template = """
Eres un asistente experto en recuperación de información. A continuación tienes una lista numerada de títulos de documentos. Después se presenta una pregunta.

Tu tarea es analizar los títulos y devolver **el número de los 3**  documentos más relevantes para responder esa pregunta. Devuélvelos en orden de mayor a menor relevancia, separados por comas.

No expliques tu razonamiento. No devuelvas los títulos. No incluyas texto adicional. Solo los números.

---

TÍTULOS:
{document_titles}

---

PREGUNTA:  
{user_question}

---

RESPUESTA:

"""

answer_prompt_template = """
Eres un asistente médico especializado en vacunas. A continuación tienes una serie de fragmentos extraídos del "Manual de Inmunizaciones de la AEP".

1. Tu objetivo es responder la pregunta del usuario utilizando únicamente la información contenida en los fragmentos.
2. Si los fragmentos no contienen suficiente información para responder a la pregunta, indica claramente que no es posible responder con los datos proporcionados.
3. Cita siempre el capítulo o fragmento que respalde cada afirmación (por ejemplo: “Según el capítulo 28. Hepatitis A, …”).
4. Al final de tu respuesta, incluye solamente las URLs de los capítulos a los que has hecho referencia, utilizando el siguiente formato:

* Capítulo 1. Título del capítulo: [https://vacunasaep.org/documentos/manual/cap-1]
* Capítulo 2. Título del capítulo: [https://vacunasaep.org/documentos/manual/cap-2]

5. No aportes información externa a estos fragmentos ni inventes datos no incluidos en ellos.

Pregunta del usuario:
---------------------
{question}

Fragmentos recuperados:
-----------------------
{context}

Respuesta:

"""
evaluation_prompt_template = """
You are a professional evaluator assessing two answers to the same professional question: one written by a human expert, and the other generated by a retrieval-augmented generation (RAG) system.

Evaluate both answers according to the following five criteria, assigning a score from 0 to 5 for each (0 = very poor, 5 = excellent). Provide a brief justification for each score.

The response should be in JSON format, containing a dictionary with two keys: "human_evaluation" and "rag_evaluation". Each of this keys should also contain a dictionary with this five keys: "correctness", "completeness", "relevance", "clarity_and_fluency", and "alignment_with_intent". Each of these keys should have a score from 0 to 5, and a justification string. Here's an example of the expected output:

{{
  "human_evaluation": {{
    "correctness": X,
    "completeness": X,
    "relevance": X,
    "clarity_and_fluency": X,
    "alignment_with_intent": X,
    "justification": "Explain the scoring briefly."
  }},

  "rag_evaluation": {{
    "correctness": X,
    "completeness": X,
    "relevance": X,
    "clarity_and_fluency": X,
    "alignment_with_intent": X,
    "justification": "Explain the scoring briefly."
  }}
}}


### Evaluation Criteria:
1. Correctness: Is the information factually accurate?  
2. Completeness: Does the answer cover all relevant aspects of the question?  
3. Relevance: Is the content directly related to the question asked?  
4. Clarity and Fluency: Is the answer easy to understand and well-written?  
5. Alignment with Intent: Does the answer respond to the specific intent behind the question?

---

### Input:

Question:  
{question}

Human Answer:  
{human_answer}

RAG Answer:  
{rag_answer}

---

### Output:

"""
